<!DOCTYPE html>
<html lang="en">

<head>
    <title>Object Detection in the Browser</title>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Jason Mayes">

    <!-- Import the webpage's stylesheet -->
    <style>
        body {
            font-family: helvetica, arial, sans-serif;
            margin: 2em;
            color: #3D3D3D;
        }

        h1 {
            font-style: italic;
            color: #FF6F00;
        }

        h2 {
            clear: both;
        }

        em {
            font-weight: bold;
        }

        video {
            clear: both;
            display: block;
        }

        section {
            opacity: 1;
            transition: opacity 500ms ease-in-out;
        }

        header,
        footer {
            clear: both;
        }

        .removed {
            display: none;
        }

        .invisible {
            opacity: 0.2;
        }

        .note {
            font-style: italic;
            font-size: 130%;
        }

        .videoView,
        .classifyOnClick {
            position: relative;
            float: left;
            width: 100%;
            margin: 2% 1%;
            cursor: pointer;
        }

        .videoView p,
        .classifyOnClick p {
            position: absolute;
            padding: 5px;
            background-color: rgba(255, 111, 0, 0.85);
            color: #FFF;
            border: 1px dashed rgba(255, 255, 255, 0.7);
            z-index: 2;
            font-size: 12px;
            margin: 0;
        }

        .highlighter {
            background: rgba(0, 255, 0, 0.25);
            border: 1px dashed #fff;
            z-index: 1;
            position: absolute;
        }

        .classifyOnClick {
            z-index: 0;
        }

        .classifyOnClick img {
            width: 100%;
        }
    </style>
</head>

<body>
    <h1>Object Detection in the Browser</h1>
    <p>
        This is a small benchmark to see how fast <a
            href="https://github.com/tensorflow/tfjs-models/tree/master/coco-ssd">coco-ssd</a> runs on TensorFlow.js in
        the browser.
        Please upload your results to <a href="https://github.com/opendatacam/prototype-object-detection-in-the-browser/">GitHub</a> or send me a message
        via <a href="https://mastodon.social/@vsaw">Mastodon</a>
    </p>
    <section id="loading">
        <h2>Loading...</h2>
    </section>
    <section id="demos" class="invisible">
        <div id="liveView" class="videoView">
            Upload Image <input type="file" id="imageUploadButton" accept="image/*">, or
            <button id="webcamButton">Enable Webcam</button>
            <span id="fps"></span> fps (reload page to switch modes)
            <video id="webcam" style="max-width: 100%;" autoplay playsinline></video>
            <img style="display:block" id="image">
        </div>
    </section>

    <!-- Import TensorFlow.js library -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.11.0/dist/tf.min.js" type="text/javascript"></script>

    <!-- Load the coco-ssd model to use to recognize things in images -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>

    <!-- Webcam -->
    <script type="text/javascript" src="https://unpkg.com/webcam-easy/dist/webcam-easy.min.js"></script>

    <!-- Import the page's JavaScript to do some stuff -->
    <script>
        /********************************************************************
         * Demo created by Jason Mayes 2020.
         *
         * Got questions? Reach out to me on social:
         * Twitter: @jason_mayes
         * LinkedIn: https://www.linkedin.com/in/creativetech
         ********************************************************************/

        const demosSection = document.getElementById('demos');

        var model = undefined;
        let orientation = undefined;

        function handleOrientationChange() {
            if(window.innerWidth > window.innerHeight) {
                console.log('Landscape');
                orientation = 'landscape';
            } else {
                console.log('Portrait');
                orientation = 'portrait';
            }
        }

        // Determine initial device orientation and react to changes.
        handleOrientationChange();
        window.onresize = handleOrientationChange;

        // Before we can use COCO-SSD class we must wait for it to finish
        // loading. Machine Learning models can be large and take a moment to
        // get everything needed to run.
        cocoSsd.load().then(function (loadedModel) {
            model = loadedModel;
            // Show demo section now model is ready to use.
            demosSection.classList.remove('invisible');
            document.getElementById('loading').style = 'display: none;';
        });

        const video = document.getElementById('webcam');
        const liveView = document.getElementById('liveView');
        const image = document.getElementById('image');
        const imageUploadButton = document.getElementById('imageUploadButton');
        const enableWebcamButton = document.getElementById('webcamButton');

        // Keep a reference of all the child elements we create
        // so we can remove them easilly on each render.
        var children = [];

        function updatePredictionsLiveView(scale, predictions, confidence) {
            for (let i = 0; i < children.length; i++) {
                liveView.removeChild(children[i]);
            }
            children.splice(0);

            // Now lets loop through predictions and draw them to the live view if
            // they have a high confidence score.
            for (let n = 0; n < predictions.length; n++) {
                // If we are over confidence sure we are sure we classified it right, draw it!
                if (predictions[n].score > confidence) {
                    const p = document.createElement('p');
                    p.innerText = predictions[n].class + ' - with '
                        + Math.round(parseFloat(predictions[n].score) * 100)
                        + '% confidence.';
                    // Draw in top left of bounding box outline.
                    p.style = 'left: ' + predictions[n].bbox[0] * scale + 'px;' +
                        'top: ' + predictions[n].bbox[1] * scale + 'px;' +
                        'width: ' + (predictions[n].bbox[2] * scale - 10) + 'px;';

                    // Draw the actual bounding box.
                    const highlighter = document.createElement('div');
                    highlighter.setAttribute('class', 'highlighter');
                    highlighter.style = 'left: ' + predictions[n].bbox[0] * scale + 'px;'
                        + 'top: ' + predictions[n].bbox[1] * scale + 'px;'
                        + 'width: ' + predictions[n].bbox[2] * scale + 'px;'
                        + 'height: ' + predictions[n].bbox[3] * scale + 'px;';

                    liveView.appendChild(highlighter);
                    liveView.appendChild(p);

                    // Store drawn objects in memory so we can delete them next time around.
                    children.push(highlighter);
                    children.push(p);
                }
            }
        }

        /*********
         * Demo 1 File Upload
         *********/
        imageUploadButton.addEventListener('change', () => {
            const curFiles = imageUploadButton.files;
            if(curFiles.length === 0) {
                return;
            }

            video.style = 'display:none;'
            enableWebcamButton.disabled = true;

            // image.src = curFiles[0];
            image.src = URL.createObjectURL(curFiles[0]);
            image.addEventListener('load', () => {
                const scale = image.clientWidth / image.width;
                model.detect(image).then(function (predictions) {
                    updatePredictionsLiveView(scale, predictions, 0.66);
                });
            });
        });

        /********************************************************************
        // Demo 2: Continuously grab image from webcam stream and classify it.
        // Note: You must access the demo on https for this to work:
        // https://tensorflow-js-image-classification.glitch.me/
        ********************************************************************/

        // Check if webcam access is supported.
        function hasGetUserMedia() {
            return !!(navigator.mediaDevices &&
                navigator.mediaDevices.getUserMedia);
        }

        // If webcam supported, add event listener to button for when user
        // wants to activate it.
        if (hasGetUserMedia()) {
            enableWebcamButton.addEventListener('click', enableCam);
        } else {
            console.warn('getUserMedia() is not supported by your browser');
        }

        // Enable the live webcam view and start classification.
        function enableCam(event) {
            if (!model) {
                console.log('Wait! Model not loaded yet.')
                return;
            }

            // Hide the button.
            // event.target.classList.add('removed');
            image.style = 'display:none;'
            imageUploadButton.disabled = true;

            // getUsermedia parameters.
            const constraints = {
                video: {
                    facingMode: "environment",
                    width: 1280,
                    height: 720,
                }
            };

            // Activate the webcam stream.
            navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {
                video.srcObject = stream;
                video.addEventListener('loadeddata', predictWebcam);
            });
        }

        frame = 0;
        setInterval(() => {
            document.getElementById("fps").innerText = frame;
            frame = 0;
        }, 1000);

        // Prediction loop!
        function predictWebcam() {
            // console.debug(video);
            let scale = video.clientWidth / 1280.0;
            if(orientation === 'portrait') {
                console.debug('using portrait scale');
                scale = video.clientWidth / 720.0;
            }

            // Now let's start classifying the stream.
            model.detect(video).then(function (predictions) {
                frame++;

                updatePredictionsLiveView(scale, predictions, 0.66);

                // Call this function again to keep predicting when the browser is ready.
                window.requestAnimationFrame(predictWebcam);
            });
        }
    </script>
</body>

</html>
